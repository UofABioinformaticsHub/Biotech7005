---
title: "Transcriptomics Practical"
author: "Steve Pederson/Jimmy Breen"
date: '16th September, 2019'
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "hide",
                      eval = FALSE,
                      message = FALSE, warning = FALSE)
```

```{r loadPackages, echo=FALSE, eval=TRUE}
library(tidyverse)
library(limma)
library(ggrepel)
library(pander)
library(RColorBrewer)
library(reshape2)
library(scales)
library(pheatmap)
library(VennDiagram)
library(edgeR)
library(DiagrammeR)
theme_set(theme_bw())
```

## Introduction

The motivation behind many RNA-Seq or transcriptomic studies is to detect the level of activity at a genomic locus, and determine if any changes are evident due to the specific biological question.
Whilst we use RNA-Seq as one of the most common data types, in this context the sequence content is secondary to the abundances of genes/transcripts.

However, sequence information is still highly important as we can use this information to:

- align reads to the genome/transcriptome
- detect splicing events
- find which transcripts are in usage
- identify novel transcripts
- detect allele specific expression patterns

In the interests of time, we will focus primarily on quantifying gene abundances today.

### Workflow summary

The basic outline of a common RNA-Seq analysis workflow is given in the following diagram.
Today we'll focus primarily on the gene-level analysis which follows the path shown on the left.
This uses the genome as the reference, with alignment performed by `hisat2`.
An alternative would be to align to the transcriptome using `kallisto`, however this is beyond the limits of what we can realistically do in a two-hour session.

```{r Pipeline, echo=FALSE, results='asis', fig.height=5, eval = TRUE}
steps <- 6
nodes <- create_node_df(
  n = steps ,
  label = c("Raw Data", "Trimmed Data", "Aligned Data", "Transcript Level\nCounts", "Gene Level\nCounts", "Downstream\nAnalysis"),
  type = "number",
  shape = "rectangle",
  style = "filled",
  fillcolor = colorRampPalette(ngsReports::pwf %>% ngsReports::getColours() %>% magrittr::extract(3:1))(steps),
  fontcolor = rgb(0, 0, 0),
  fontsize = 12,
  width = 1.5
)
edges <- create_edge_df(
  from = c(1, 2, 2, 3, 4, 5),
  to = c(2, 3, 4, 5, 6, 6),
  rel = "leading_to",
  label = c("Adapter Removal\nQuality Trimming", "HISAT2", "kallisto", "featureCounts"),
  fontsize = 10
)
graph_attrs <- c("layout = neato", "overlap = FALSE", "outputorder = nodesfirst")
create_graph(nodes_df = nodes, edges_df = edges, 
             attr_theme = list(graph_attrs= graph_attrs)) %>%
  render_graph()
```

## VM Setup

### Today's Data

Today we will perform the initial steps on your VM, with the later `R` sections performed on your local machine.

Today's dataset has been taken from https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-4119/, and this represents a set of reads from the human Chromosome 1, with known gene expression patterns.
This came from an analysis in which multiple analytic methods were investigated and the full paper is available at https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4712774/.

**Log onto your VM**

Enter the following commands carefully. Cut & Paste is fine, but remember you'll need to use `Ctrl+Shift+V` to paste into the terminal on your VM.
You will be prompted for a password, and this is `biotech7005`.

```
cd ~
scp -r biotech7005@10.150.8.124:/home/biotech7005/RNA_Practical ./
```

As you may be able to tell, this will copy the files from a spare VM to your own, so this should be fairly quick.
This should also have copied across the correct file paths for beginning the tasks today.

Now let's just check our installations first to make sure nothing has gone wrong during setup.
If you don't get the expected results, call Steve over.

### HISAT2

The aligner we'll use today is called `hisat2` and it's an aligner which is splice aware, meaning it's able to handle alignments which contain sequences from two separate exons.
Let's check that it's installed

```
whereis hisat2
```

This should give you the output `/home/biotech7005/miniconda/bin/hisat2`

### Feature Counts

We'll also need the software `featureCounts` from the `Subread` set of tools.

```
whereis featureCounts
```

This should give `/home/biotech7005/miniconda/bin/featureCounts`


## R

We already have `R` installed on your VM and this can be accessed by opening your favourite internet browser (i.e. Firefox, Chrome, Safari) on your local laptop.
In the address bar go to: `your.ip.address:8787`

*Make sure you enter your actual IP address (10.150.8.xx)*

### Create an R Project for This Practical

Earlier, we made a folder called `RNA_Practical` when we copied the data across.
Let's make an R Project for this folder

1. `File` > `New Project`
2. Select `Existing Directory`
3. Click the `Browse` button
4. Navigate to the `RNA_Practical` folder and select `Choose`
5. Click `Create Project`


**That's everything done for the setup!!!**

## RNA Seq Analysis

### Raw Data

By now, your data will have completed downloading.
Here we have our 6 samples, with 3 from `sampleA` and another 3 from `sampleB`.
Let's run FastQC to make sure they're acceptable.

#### FastQC

```
cd ~/RNA_Practical/01_rawData
mkdir FastQC
fastqc -o FastQC fastq/*gz
```

As we have logged onto our VM using RStudio, we can inspect these reports directly in our browser.
Using the Files pane in RStudio, navigate to the `FastQC` directory and click on one of the `.html` files.
When asked, choose `View in Web Browser`

These should all look pretty good, but we can get a few pieces of key information about these files.

**How many reads are in each file?**  
**How long are the reads?**

### Genome Information

For this section, we need Sequence (fasta) and Gene Descriptions(gtf) files

For today, we'll need the sequence from Chromosome 1 and a `gtf` which corresponds.
We'll get these from Ensembl, so cut and paste the following code directly into your VM.

```
cd
mkdir -p genomes/Hsapiens
cd genomes/Hsapiens
wget ftp://ftp.ensembl.org/pub/release-93/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.1.fa.gz
gunzip Homo_sapiens.GRCh38.dna.chromosome.1.fa.gz
wget ftp://ftp.ensembl.org/pub/release-93/gtf/homo_sapiens/Homo_sapiens.GRCh38.93.chr.gtf.gz
```

You may have already realised that this `gtf` contains all the information for the entire genome, so we'll need to create one containing just the gene information for `chr1`.

First we'll copy the file header.

```
zcat Homo_sapiens.GRCh38.93.chr.gtf.gz | egrep "^#!" > Homo_sapiens.GRCh38.93.chr1.gtf
```

Then we'll just get the gene information for chr1.
he first field in a `gtf` file is the chromosome, we this will be easy to extract using `egrep`

```
zcat Homo_sapiens.GRCh38.93.chr.gtf.gz | egrep "^1\s" >> Homo_sapiens.GRCh38.93.chr1.gtf
```

To avoid confusing ourselves, let's delete the full gtf so we only have the one.

```
rm Homo_sapiens.GRCh38.93.chr.gtf.gz
```

### Aligning our data

Before we align our data, we need to create an index of the genome.
This basically allows the algorithm to perform the alignments quickly.
This always needs to be performed by the aligner that we'll be using, and today we'll use `hisat2`.

In order to create a list of splice sites, we'll first create a list of these using a script supplied with `hisat2` based on those in the gtf file 

```
cd ~/genomes/Hsapiens
hisat2-build Homo_sapiens.GRCh38.dna.chromosome.1.fa Homo_sapiens.GRCh38.dna.chromosome.1
```

This will take about 5 minutes to complete for chromosome 1, and considerably longer if we chose the entire human genome.

Once this process is complete, we'll need to align each of our files to the indexed genome.
To do this we'll have to write a script.

First we'll create all the directories we need:

```
cd ~/RNA_Practical
mkdir -p 02_aligned/sam
mkdir -p 02_aligned/bam
mkdir -p 02_aligned/logs
touch runAlignments.sh
nano runAlignments.sh
```

This list line will open the empty script in `nano` for you, so cut & paste the following code into the open script.

```
#!/bin/bash
FQDIR=~/RNA_Practical/01_rawData/fastq
SAMDIR=~/RNA_Practical/02_aligned/sam
LOGDIR=~/RNA_Practical/02_aligned/logs
BAMDIR=~/RNA_Practical/02_aligned/bam
IDX=~/genomes/Hsapiens/Homo_sapiens.GRCh38.dna.chromosome.1

for f in ${FQDIR}/*gz
  do
    echo -e "Running alignment on ${f}"
    SAM=${SAMDIR}/$(basename ${f} _1.fq.gz).sam
    BAM=${BAMDIR}/$(basename ${f} _1.fq.gz).sorted.bam
    hisat2 \
    -x ${IDX} \
    -p 3 \
    -U ${f} \
    -S ${SAM} &> ${LOGDIR}/$(basename ${f} _1.fq.gz).log
    echo -e "done"
    echo "Converting ${SAM} to ${BAM}"
    samtools view -bh ${SAM} | samtools sort -o ${BAM}
    rm ${SAM}
  done

```

To exit & save the script hit `Ctrl+x` followed by `y`.

Now we need to make this script executable so enter.

```
chmod +x runAlignments.sh
./runAlignments.sh
```

These alignments will take quite a while, so while we're waiting, go to the hisat2 page https://ccb.jhu.edu/software/hisat2/manual.shtml

#### Questions

1. Explain what the following parameters meant during the call to hisat2
    a. `-x ${IDX}`
    b. `-p 3`
    c. `-U ${f}`
2. How could we have specified splice sites?
3. What does the `\` symbol mean in the above script
4. `hisat2` only outputs data as a .`sam`format so we've converted this to a .`bam` format as part of the script. As part of this, we've used the pipe symbol `(|)`. 
    a. What does this second step do to our data? 
    b. Why might this be an advantage
5. Once at least one sample has finished being aligned, have a look at the log files. Can you interpret the output?
6. Do you have any suggestions for improving the above script?

### Counting Alignments

Once we've aligned our data, we need to count how many reads have aligned to each gene, and we'll do this using the software `featureCounts`.
This is the fastest part of today's practical, especially as we sorted our `bam` files.
Again, we'll write a script to perform this step

First we'll create all the directories we need:

```
cd ~/RNA_Practical
mkdir -p 02_aligned/counts
touch countAlignments.sh
nano countAlignments.sh
```

```
#!/bin/bash
BAMDIR=~/RNA_Practical/02_aligned/bam
COUNTDIR=~/RNA_Practical/02_aligned/counts
GTF=~/genomes/Hsapiens/Homo_sapiens.GRCh38.93.chr1.gtf

# Find the entire list of files
BAMS=`find ${BAMDIR} -name "*sorted.bam" | tr '\n' ' '`

# Count all files in a single process
featureCounts \
  -Q 10 \
  -T 3 \
  -a ${GTF} \
  -o ${COUNTDIR}/counts.out ${BAMS}
```

## RNA Seq Analysis using `R`

### Loading the data into `R`

The above script will create a file with a single header line, followed by column names and the data.
Some of the columns are not relevant for us today, so we'll first load the file, then remove all the columns which are not required.

(If your alignments are not ready, the file can be found [here](counts.out))

However, first we need to load the `R` packages for differential gene analysis.

*If you'd like to directly download the data from your VM instead of using the provided file, install FileZilla for easy data transfer between your local laptop and the VM (see the end of this page)*

```{r}
library(tidyverse)
library(edgeR)
library(limma)
theme_set(theme_bw())
```

```{r}
file <- "counts.out"
counts <- read_delim(file, comment = "#", delim = "\t")
```

Columns 2 to 6 are not particularly relevant for us today.
The counts for each gene are in columns 7 & beyond, whilst the first column contains the gene IDs.

The type of object we like to use in `R` is known as a `DGEList` and we'll need to set our data up as this object type before being able to do any meaningful analysis.

First we'll select the columns we need, then we'll change this into a matrix, which is what the `DGEList` objects need

```{r}
dgeList <- counts %>%
  dplyr::select(Geneid, ends_with("bam")) %>%
  as.data.frame() %>%
  column_to_rownames("Geneid") %>%
  as.matrix() %>%
  DGEList()
```

Obviously these sample names are not convenient, so let's edit these a little.
In `DGEList` objects, the sample names are also the column names.

```{r}
colnames(dgeList) <- basename(colnames(dgeList))
colnames(dgeList) <- gsub(".sorted.bam", "", colnames(dgeList))
```

Let's have a look at the object

```{r}
dgeList
```

You can see that we have the counts for every gene in a component called `dgeList$counts`, and information about the samples in a component called `dgeList$samples`.

Let's add a column to the `$samples` component which defines which sample group we are in.

```{r}
dgeList$samples$type <- gsub("sample([AB]).*", "\\1", rownames(dgeList$samples))
dgeList$samples$type <- as.factor(dgeList$samples$type)
dgeList$samples$group <- as.integer(dgeList$samples$type)
```

In the second line above, we converted the text to a categorical variable (i.e. a `factor`), and then we used the category number to assign the values in the `group` column.

Notice in the `$samples` component, there is also a column called `lib.size`.
This is the total number of reads aligned to genes within each sample.

Let's see if we have much difference between samples & groups?

```{r}
dgeList$samples %>%
  rownames_to_column("sample") %>%
  ggplot(aes(x = sample, y = lib.size / 1e6, fill = type)) +
  geom_bar(stat = "identity") +
  ylab("Library size (millions)")
```

In today's data, we don't have a huge difference, but this can vary greatly in many experiments.
When analysing counts, the number of counts will clearly be affected by the total library size (i.e. the total number of reads counted as aligning to genes).
We before passing this data to any statistical models, we need to calculate a scaling factor that compensates for this.

```{r}
dgeList <- calcNormFactors(dgeList)
```

Notice that now the column `norm.factors`is no longer all `1`.
This is used by all downstream functions in `edgeR`.

### Data Exploration

One of the things we look for in a dataset, is that the samples from each treatment,group together when we apply dimensional reduction techniques such as Principal Component Analysis (PCA) or Multi Dimensional Scaling (MDS).
The package `edgeR` comes with a convenient function to allow this.

```{r}
plotMDS(dgeList, labels = dgeList$samples$type, col = dgeList$samples$group)
```

Here we can see a clear pattern of separation between the groups.

(If we don't see this, it may mean we have some difficult decisions to make.
Maybe we need to check our samples for mislabelling?
Maybe there is some other experimental factor which we're unaware of.)


An interesting way to view our data is to use the value known as `Counts per million`.
This accounts for difference in library sizes and gives an estimate of how abundant each gene is.

```{r}
head(cpm(dgeList))
```

You can see from this first 6 genes, that we have one highly abundant gene, and a couple with far lower expression levels.

### Filtering out genes

Genes with low count numbers give us minimal statistical power to detect an changes in expression level, so a common approach is to filter out these genes.
This reduces the issues which we will face due to multiple hypothesis testing, effectively increasing the statistical power of a study.

A common method would be to remove any genes which are below a specific CPM threshold in the majority of samples.
In this dataset, we might like to remove genes which have $<1$ CPM in 3 or more samples.
As we have 3 samples in each group, this means we need genes to have greater than about 10 reads aligning to them, in each sample from at least one group.
Any number of strategies can be applied for this stage.

We can plot the densities of each of our samples using log-transformed CPM values, and the clear peak in the range of very low expression is clearly visible.

```{r}
plotDensities(cpm(dgeList, log = TRUE))
```

Let's filter our dataset, and remove genes with low abundances.

```{r}
genes2keep <- rowSums(cpm(dgeList) > 1) > 3
```

Here we've created a logical vector defining which genes to keep.
To get a quick summary of this enter

```{r}
summary(genes2keep)
```

Now let's look at the densities after filtering.

```{r}
plotDensities(cpm(dgeList, log = TRUE)[genes2keep, ])
```

Now we're happy that we have the genes we can extract meaningful results from, let's remove them from our dataset.

```{r}
dgeList <- dgeList[genes2keep, ]
```


### Calculating moderated dispersions

Most RNA-Seq analysis is performed using the *Negative Binomial Distribution*.
This is similar to a *Poisson* distribution, where we **count** the number of times something appears in a given interval.
The difference with a Negative Binomial approach is that we can model data which is more variable.
Under the Poisson assumptions, the variance equals the mean, whilst under the *NB* this is no longer required.

This extra variability is known as the *dispersion*, and our estimates of dispersion will be too high for some genes, and too low for others.
Taking advantage of the large numbers of genes in an experiment, we can shrink the ones that are too high, and increase the ones that are too small.
This is an important step in RNA Seq analysis, as it reduces the number of false positives, and increase the numbers of true positives.

Before we do this, we need to define our statistical model.
Here, our term of interest is in the column `type`.

```{r}
design <- model.matrix(~0 + type, data = dgeList$samples)
design
```


```{r}
dgeList <- estimateDisp(dgeList, design = design)
```

### Performing differential expression analysis

The most common analytic approach we use is an Exact Test, proposed by Robinson and Smyth.
This is analogous to Fisher's Exact Test, as once again we are dealing with count data, not Normally distributed data.

In the following line of code, we are comparing the first two groups in our data.
Clearly we only have two groups in this dataset,but it is quite common to have multiple groups in other analyses.

```{r}
etResults <- exactTest(dgeList, pair = 1:2)$table 
```

As we've discovered, we need to account for our multiple testing considerations.
Under the null hypothesis, we would expect the distribution of $p$-values to be approximately uniform on the interval [0, 1]
.
However, under the the alternative hypothesis, we should see a spike of $p$-values near zero.
A "healthy" dataset will likely have a mixture of the two distributions, so we should see a flat distribution, with a cluster of values near zero.

```{r}
hist(etResults[,"PValue"], breaks= 50)
```

This gives us confidence that we will be able to detect DE genes in this dataset.
Now, we can proceed to estimate the False Discovery Rate in this data.

```{r}
etResults <- etResults %>%
  rownames_to_column("Geneid") %>%
  mutate(FDR = p.adjust(PValue)) %>%
  arrange(PValue) %>%
  as_data_frame()
```

Find the highest ranked gene in this dataset
If it's been given a negative value for logFC, this means it's down-regulated in the second of the two conditions.
The opposite is true for a positive value for logFC.
Let's check the raw counts using *counts per million*

```{r}
cpm(dgeList, log= TRUE)["ENSG00000117472",]
```

Inspect a few more of the highly ranked genes, so make sure you can understand these results.

Let's get a list of significant genes, by using an FDR of 0.01, and logFC $>1$.
(logFC is fold-change on the $\log_2$ scale, so that logFC=1, is a two-fold increase, logFC=2 is a 4-fold increase,and logFC=-1 is a halving of expression)

```{r}
sigGenes <- filter(etResults, FDR< 0.01, abs(logFC) > 1)$Geneid
```

Now we can visualise the pattern of logFC to expression level.

```{r}
plotMD(dgeList, status = rownames(dgeList)%in%sigGenes)
```

An alternative is to plot the logFC in relation to the $p$-value, to make what is known as a volcano plot.

```{r}
etResults %>%
  mutate(DE = Geneid %in% sigGenes) %>%
  ggplot(aes(x = logFC, y = -log10(PValue),colour = DE)) +
  geom_point() +
  scale_colour_manual(values = c("grey50", "red"))
```

